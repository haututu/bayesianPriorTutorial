---
title: "Tutorial for the construction of informative Bayesian priors: Leveraging information from large samples to improve estimation of effects with small samples"
author: "Taylor Winter, Benjamin Riordan, Anthony Surace, & Paul Jose"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
library(tidyverse)
opts_chunk$set(echo = TRUE, message=FALSE)

theme_set(theme_classic())
```


## Overview

This document is a supplementary tutorial associated with a paper published in the journal Addiction[^1] and the accompanying R code placed on [GitHub](https://github.com/haututu/bayesianPriorTutorial). In our published paper, we used a large dataset to investigate if sexual minorities (SMs) are at higher risk of psychological distress relative to heterosexuals. On the basis of the parameters derived from the large dataset, we construct priors and then produced a similar model using a smaller different dataset. Lastly, we use the priors to inform part of a mediation analysis to obtain a more reliable understanding on how much stress can mediate the increased likelihood of hazardous drinking expressed by SMs relative to heterosexuals in Aotearoa New Zealand.

In the associated manuscript, we used the [National Survey on Drug Use and Health](https://nsduhweb.rti.org/respweb/homepage.cfm) (NSDUH; n = 83,661) to inform our smaller survey, the [New Zealand Health Survey](https://www.health.govt.nz/nz-health-statistics/national-collections-and-surveys/surveys/new-zealand-health-survey) (NZHS, n=24098). In the former case, the dataset is large and publicly available. In the latter, the data is provided by applying to the New Zealand Ministry of Health. For the purposes of this tutorial, we have created smaller datasets that show similar trends to data presented in our associated manuscript, data does _not_ use real cases from the surveys in order to protect confidentiality (See `makeSyntheticData.R` for details on how we created these datasets). As we have used fabricated New Zealand data based on our findings in the primary manuscript, the effects do not align exactly and the _estimates provided herein are for demonstration purposes only_.

The analysis relies on the `tidyverse` and a Bayesian regression package called `brms`[^2]. The present document, constrained for brevity, does not describe these packages, and we would encourage readers to consult the `brms` vignettes or any basic R tutorial for `tidyverse` walkthroughs.

[^1]: Winter, T., Riordan, B., Surace, A., & Jose, P. (In Review). A comparison of Bayesian and classical statistics for substance research with sexual minorities

[^2]: Paul-Christian BÃ¼rkner (2017). brms: An R Package for Bayesian Multilevel Models Using Stan. Journal of
  Statistical Software, 80(1), 1-28. doi:10.18637/jss.v080.i01

## Modelling the United States data (NSDUH)

We start with the script named `usModel.R`, by loading the necessary libraries and the `syntheticUs.RDS` file. You can now check and plot the data if you wish. The data contains five variables:

1. SEXIDENT - Sexual identity; Heterosexual (1), Homosexual (2), or Bisexual(3)
2. AGE2 - An ordinal variable constituted by age groupings (labels attached to variable)
3. IRFAMIN3 - Total family income as an ordinal variable (labels attached to variable)
4. IRSEX - Categorical sex variable: either male (1) or female (2)
5. SPDMON - Is a binomial variable denoting an experience of psychological distress in the last six months, either no stress (0) or stress (1)

```{r readUs}
library(tidyverse)
library(brms)

syntheticUs <- readRDS("data/syntheticUs20191204.RDS")
```

### Constructing priors

We then construct simple weak priors for the logistic regression, and we do this out of necessity because the dataset is so large that it would take a day to run. When using Stan as your MCMC sampler, even very weak priors can have a major impact on your convergence time without influencing your posterior to any appreciable degree.

We specify priors as normal distributions, for example the coefficient `IRSEX2` has a normal distribution with a mean of one and standard deviation of one (note the scale is stipulated to be log odds for logistic regression). These constraints mean we believe the odds of females having higher stress than males has 95% probability of falling between an odds of 0.14 and 7.38... it should be reasonable to assume the odds of stress for females relative to males falls within such a large range of odds. In fact, it should make good sense to rule out odds ratios outside of these bounds. Where a prior is not specified, we have a prior for class `b` as a catch all for the remaining coefficients in the model.

A further point one may note below, is that the age and income variables start with `mo~`. This prefix is short for 'monotonic', which is a type of mathematical function that allows one to model ordinal data. In short, a monotonic effect tries to model a slope for an ordinal variable, then models how much each category deviates from the regression slope. As we are using monotonic functions in our `brms` model, our variables are given the `mo~` prefix. Similarly, as sex is a categorical variable, i.e., the effect of females is modeled in reference to males and we therefore add the suffix `~2` because females are coded as a `2` in the US data.


```{r priorsUs}
priors <- c(prior("normal(0, 2)", class="b"),
            prior("normal(1, 1)", coef="IRSEX2"),
            prior("normal(-1, 1)", coef="moAGE2"),
            prior("normal(-1, 1)", coef="moIRFAMIN3"))
```

### Basic model

We can now model the US data. If you're familiar with `lme4` syntax, then this is quite straight forward, we model stress (SPDMON) as a covariate with age (AGE2), sex (IRSEX), SM status (sexident), and family income (IRFAMIN3) as predictors. As mentioned, age and family income are ordinal variables and we capture this relationship by modelling a  monotonic effect, implemented by wrapping the variable with the `mo()` function.

We then specify the family (response distribution), which is the distribution we believe the dependent variable will take. In this case it is a binomial variable which is best represented by the Bernoulli distribution (analogous to a traditional logistic regression). We parse our priors, and want to sample four chains across four CPU cores. It will take about five minutes to run (Feel free during this time to reflect on our pain when running a sample of over 80,000, which takes considerably longer to compute).

```{r modelUs, cache=TRUE}
brms.us <- brm(
  SPDMON ~ mo(AGE2) + IRSEX * SEXIDENT + mo(IRFAMIN3),
  family = bernoulli(),
  prior = priors,
  cores = 4,
  chains = 4,
  data=syntheticUs
  )
```

Finally, we can see the results of our logistic regression using the `summary(brms.us)` function. It will output a number of simplex parameters which pertain to our monotonic effects, it is okay to ignore them and focus on the population-level effects for the purpose of this walkthrough. The population-level effects will form the basis of our priors for the NZ data, extracted in the table below (Table 1).

```{r fixef, echo=FALSE}
fixef(brms.us) %>%
  kable(caption = "<b>Table 1.</b> Fixed effects for the logistic regression model using US data") %>%
  kable_styling(full_width = FALSE)
```

As a couple of added tips, the extracted table is produced using `fixef(brms.us)` and the coefficients are, of course, logits. If you wanted to produce odds, you can wrap the aforementioned function with an exponential function `exp(fixef(brms.us))`. You can also produce graphs of the marginal effects in the regression model using `marginal_effects(brms.us)`.

## Modelling New Zealand data (NZHS)

We will break the following section up by first constructing the priors, running the same model in the NZ data that we presented above, then running a more complex analysis (i.e., mediation). The code we are describing is presented in the `nzModel.R` script. You can start by loading the NZ data with the following code.

```{r}
syntheticNz <- readRDS("data/syntheticNz.RDS")
```

### Constructing priors

In our paper, we discuss using the coefficients and error terms from our US sample as priors for the NZ sample. However, we acknowledge that the two countries are quite different and we therefore decrease our certainty of the US-based priors by doubling the error terms. We present the original priors used in our paper, but you can easily adjust them to the priors in the sample data or any other reasonable prior you have in mind. You can multiply the error terms easily with `fixef(brms.us)[,2] * 2`.

```{r priorsNz}
priors <- c(
  prior("normal(0, 2)", class="b"),
  prior("normal(-0.19, 0.07)", coef="age"),
  prior("normal(0.28, 0.17)", coef="sexF"),
  prior("normal(-0.15, 0.05)", coef="nzdep2013"),
  prior("normal(0.87, 0.45)", coef="sexidentGayDLesbian"),
  prior("normal(1.18, 0.31)", coef="sexidentBisexual"),
  prior("normal(0.09, 0.56)", coef="sexF:sexidentGayDLesbian"),
  prior("normal(0.08, 0.40)", coef="sexF:sexidentBisexual")
)
```

It's worth noting that the NZ sample has a slightly different structure and one additional variable (hazardous drinking status):

1. sexident - Sexual identity; Heterosexual, Homosexual, or Bisexual
2. age - Continuous years of age
3. nzdep2013 - A decile score for the house the synthetic respondent comes from (higher is better)
4. sex - Categorical variable either male (M) or female (F)
5. k10high - Is a binomial variable denoting an experience of psychological distress in the last six months, either no stress (0) or stress (1)
6. hazardous - Is a binomial variable referring to whether someone reported a hazardous level of drinking

### Basic model

Our basic NZ model looks almost exactly the same as our US model. However, we have also used the option `sample_prior = "yes"`, which will create a distribution for our prior. Doing so is helpful for determining how much your posterior is being influenced by the prior. In the supporting R script, you will also see we have a naive model which will perform similarly to a classical logistic regression. This allows us to compare our informed analysis to a non-informed/naive analysis to understand how much credibility we gain from the use of informed priors[^3].

[^3]:You may also note that our naive model used the prior, `prior("normal(0, 2)", class = "b")`. Similar to the US model, this prior increases sampling efficiency to reduce run time. Interestingly, you would implement regularization in a similar way but typically with a smaller standard deviation. This information can be very useful when model separation is observed in logistic regression or when you have a large number of parameters.

```{r modelNz, cache=FALSE}
brms.nz <- brm(
  k10high ~ age + sex * sexident + nzdep2013,
  family = bernoulli(),
  prior = priors,
  sample_prior = "yes",
  cores = 4,
  chains = 4,
  data=syntheticNz
  )
```

```{r modelNz.naive, cache=FALSE, include=FALSE}
brms.nz.naive <- brm(
  k10high ~ age + sex * sexident + nzdep2013,
  family = bernoulli(),
  prior = prior("normal(0, 2)", class = "b"),
  cores = 4,
  chains = 4,
  data=syntheticNz
  )

```

In order to compare differences, we have created a plot with the non-informed and informed coefficients. We can see that the credibility intervals are much smaller when we use informed priors, even though we inflated the error terms on our priors. You can investigate the coefficients between both models using `summary()` or see the rmarkdown document in the GitHub repository to see how we produced the plot below (intermediate knowledge of `tidyverse` required).

**Figure 1.** Comparison of coefficients from informative and non-informative models of the synthetic NZ sample

```{r compareGraph, echo=FALSE}
nonInformed <- fixef(brms.nz.naive) %>% 
  as.data.frame() %>% 
  mutate(
    Variable = rownames(.), 
    Model = "non-informed"
    )

fixef(brms.nz) %>% 
  as.data.frame() %>% 
  mutate(
    Variable = rownames(.), 
    Model = "informed"
    ) %>%
  bind_rows(nonInformed) %>%
  filter(Variable != "Intercept") %>%
  ggplot(aes(x=Variable, y=Estimate, ymin=Q2.5, ymax=(Q97.5), group=Model, color=Model, linetype=Model, shape=Model)) +
  geom_point() +
  geom_errorbar() +
  coord_flip()
```

Figure 1. indicates how much influence our priors have. If the midpoints of our coefficients exhibit large shifts between the informed and non-informed models, we may want to adjust our priors by increasing the error. Another way to visualize the effect of the priors is using the `hypothesis()` function in the `brms` package shown in Figure 1. You can test your assumption that an effect has over a given probability, or is higher than some other distribution (e.g., an alternatively hypothesized model). For example, below we plot the prior and posterior distributions for the effect of homosexuals having greater than log odds of zero in regards to their risk of stress. In Figure 1, we see the informed coefficient and its credible interval sit within the non-informed credible interval, then in Figure 2, we see the posterior sits very similar to the prior. We may also consider it evidence that the prior is not being too informative because the posterior has less variance than the prior, as opposed to the posterior taking the same form of the prior. 

**Figure 2.** Posterior distribution for the effect of homosexuality on the log odds of stress relative to heterosexuals

```{r priorGraph}
plot(hypothesis(brms.nz, "sexidentGayDLesbian > 0"))
```

Any other effects can be tested in the same way, and you can also conduct a sensitivity analysis by re-running models with different priors. Running a range of different priors will give richer information into how the posterior is being affected by our prior information in the context of your subsequent (smaller) dataset.

### Mediation model

When using informed priors, your models do not necessarily need to be identical. We have already mentioned that the variables can differ conceptually, such as family income in the US data and deprivation index in the NZ data. However, we can go further and apply information from a simple logistic regression model (US model) and apply it to a more complex structural equation model, e.g., a simple mediation (Figure 3).

**Figure 3.** Simple mediation for the effect of SM status on hazardous drinking with stress as a mediator.

<center>![mediationModel](walkthroughDocument_files/mediation.svg)</center>

In this example, we wanted to determine if, and to what extent, stress could mediate the relationship between SM status and hazardous drinking in the New Zealand population. We calculated the mediation effect using the Sobel product of coefficients (`a` path multiplied by `b` path). Our US data can inform our `b` path and its covariates, which can then decrease the variance in our mediation effect.

In `brms` we can include multiple regressions within the same model by wrapping the `bf()` function around each equation. We can run a simple mediation by using the following layout.

```{r exampleMed, eval=FALSE}
brm(
  bf(outcome ~ mediator + treatment + covariates) + 
    bf(mediator ~ treatment + covariates),
  priors = priors,
  family = distribution(),
  data = df
  )
```

In the NZ mediation model, we can use the same priors used previously except we need to specify which equation the priors address, using the `resp` argument.

```{r appliedMed, cache=FALSE}

# Same priors, using the 'resp' argument
priors_mediation <- c(
  prior("normal(0, 2)", class="b", resp="hazardous"),
  prior("normal(0, 2)", class="b", resp="k10high"),
  prior("normal(-0.19, 0.07)", coef="age", resp="k10high"),
  prior("normal(-0.28, 0.17)", coef="sexF", resp="k10high"),
  prior("normal(-0.15, 0.05)", coef="nzdep2013", resp="k10high"),
  prior("normal(0.87, 0.45)", coef="sexidentGayDLesbian", resp="k10high"),
  prior("normal(1.18, 0.31)", coef="sexidentBisexual", resp="k10high"),
  prior("normal(0.09, 0.56)", coef="sexF:sexidentGayDLesbian", resp="k10high"),
  prior("normal(0.08, 0.40)", coef="sexF:sexidentBisexual", resp="k10high")
)

# Mediation model using NZ data
brm.mediation <- brm(
  bf(hazardous ~ k10high + age + sex * sexident + nzdep2013) +
    bf(k10high ~ age + sex * sexident + nzdep2013),
  prior = priors_mediation,
  family = bernoulli(),
  chains = 4,
  cores = 4,
  data = syntheticNz
)
```

Now that we have our results, it is relatively straight forward to estimate a mediating effect (skip ahead if you want a mediating effect but already know or do not want to know the details). Bayesian models with MCMC will produce a large number of samples with form the posterior distribution. If we multiple the samples from our `a` path and `b` path, then we have a posterior distribution representing our mediation effect (`ab`). The direct effect is simple the `c'` path, meaning the total effect, `c`, is `ab` + `c`. If we divide our mediation effect by the total effect, we can get the proportion mediated (`ab` / `c`). This is a very similar process to a bootstrap approach within a frequentist framework.

In summary...

* `a` is the coefficient for the effect of SM status on stress
* `b` is the coefficient for the effect of stress on hazardous drinking
* `c'` is the coefficient for the effect of SM status on hazardous drinking _while controlling for stress_, and is often called the **direct effect**

We can use the MCMC samples to estimate distributions for each effect as follows...

* `ab` is just samples for `a` and `b` multiplied together and gives the **indirect effect**
* `c` is derived by adding `ab` and `c` to give the **total effect**
* **proportion mediated** is calculated by dividing the **indirect effect** by the **total effect**

In practice, we can use a convenient function form the `sjstats` package that will calculate everything for us. As sexual identity is categorical, we need to enter each separately.

```{r mediatingEffect, message=FALSE}
sjstats::mediation(
  brm.mediation,
  treatment = "sexidentGayDLesbian",
  mediator = "k10high",
  prob = 0.95)

sjstats::mediation(
  brm.mediation,
  treatment = "sexidentBisexual",
  mediator = "k10high",
  prob = 0.95)
```

We now run a naive analysis and constructed some code to convert our outputs to a dataframe for graphing. We see in the output below (Figure 4), the 95% credible interval and median for each effect is plotted separately and compared between non-informed and informed models. Unsurprisingly, we see no difference in the direct effect, which had no priors. However, there is a large reduction in the credible intervals for the indirect effect. Recall we had priors for the `a` path, and the indirect effect is the product of the `a` and `b` paths. It should also be intuitive that our total effect and proportion mediated are also reduced for a similar reason.

```{r naiveMediation, cache=FALSE, include=FALSE}
brm.mediation.naive <- brm(
  bf(hazardous ~ k10high + age + sex * sexident + nzdep2013) +
    bf(k10high ~ age + sex * sexident + nzdep2013),
  prior = prior("normal(0, 5)", class="b"),
  family = bernoulli(),
  chains = 4,
  cores = 4,
  data = syntheticNz
)
```

**Figure 4.** Resulting effects from mediation analysis comparing informed to non-informed estimates for both homosexuals and bisexuals relative to heterosexuals.

```{r plots, echo=FALSE}
mediation_data <- bind_rows(
  sjstats::mediation(
  brm.mediation,
  treatment = "sexidentGayDLesbian",
  mediator = "k10high",
  prob = 0.95) %>%
  data.frame() %>%
  mutate(identity = "homosexual",
         model = "informed"
         ),
sjstats::mediation(
  brm.mediation,
  treatment = "sexidentBisexual",
  mediator = "k10high",
  prob = 0.95) %>%
  data.frame() %>%
  mutate(identity = "bisexual",
         model = "informed"
         ),
sjstats::mediation(
  brm.mediation.naive,
  treatment = "sexidentGayDLesbian",
  mediator = "k10high",
  prob = 0.95) %>%
  data.frame() %>%
  mutate(identity = "homosexual",
         model = "non-informed"
         ),
sjstats::mediation(
  brm.mediation.naive,
  treatment = "sexidentBisexual",
  mediator = "k10high",
  prob = 0.95) %>%
  data.frame() %>%
  mutate(identity = "bisexual",
         model = "non-informed"
         )
)

mediation_data %>%
  filter(effect != "mediator") %>%
  ggplot(aes(x=identity, y=value, ymin=hdi.low, ymax=hdi.high, group=model, color=model, linetype=model)) +
  geom_point() +
  geom_errorbar() +
  facet_wrap(~effect,
             scales="free_y")
```

## Conclusion

In this walkthrough, we have given a brief overview on how to run a basic Bayesian logistic regression, and use results to inform a similar analysis on a smaller sample. The method is quite generalizable, in that you could run a regular regression model by using `normal()` instead of `bernoulli()` and could use any combination of covariates and levels of informativeness. In a more thorough approach, however, we would also advise conducting a sensitivity analysis. In a sensitivity analysis, you can use different levels of informativeness to understand the impact of your priors, and it is simple, albeit time consuming to complete. When conducting your own study, produce the non-informed analysis at a minimum and then you could also consider attaching the sensitivity analyses as a supplementary or make it available via OSF or GitHub (the former if you wish to include the models which can be quite large).

Consult the short `R` code we have made available and tweak it to understand what is happening. Also use summary functions such as `marginal_effects()` and `summary()` to further investigate models produced from our sample data.